{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env OMP_NUM_THREADS = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas_profiling\n",
    "from pandas_profiling import ProfileReport\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, confusion_matrix, make_scorer, f1_score,accuracy_score, cohen_kappa_score, log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.utils import resample\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"Bankruptcy_data_Final.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "data.info()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the first column Year\n",
    "data = data.drop(['Data Year - Fiscal'],axis=1)\n",
    "\n",
    "#Filling na with 0\n",
    "data = data.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bk=data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Scaling the numerical features\n",
    "#Scaling Data - Standardization \n",
    "#Data is scaled so that all numbers can be viewed at one standard level\n",
    "\n",
    "#First step - remove \"target variable\" from data as we do not need to scale target variable\n",
    "X = df_bk.copy()\n",
    "X = X.drop(['BK'],axis=1)\n",
    "\n",
    "#Scaling the numerical features\n",
    "scaler = StandardScaler()\n",
    "features = list(X.select_dtypes(include=np.number).columns)\n",
    "X[features] = scaler.fit_transform(X[features])\n",
    "\n",
    "#Adding taregt variable purchase back to the scaled data\n",
    "X['BK']=df_bk['BK']\n",
    "\n",
    "df_bk=X.copy()\n",
    "df_bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Enginnering ratios\n",
    "df_bk['ratio1']=df_bk['Liquidity']*df_bk['Profitability']\n",
    "df_bk['ratio2']=df_bk['Leverage Ratio']*df_bk['Asset Turnover']\n",
    "df_bk['ratio3']=df_bk['Profitability']*df_bk['Productivity']\n",
    "df_bk['ratio4']=df_bk['Operational Margin']*df_bk['Asset Turnover']\n",
    "df_bk['ratio5']=df_bk['Return on Equity']*df_bk['EPS']\n",
    "df_bk['ratio6']=df_bk['Return on Equity']*df_bk['Operational Margin']\n",
    "\n",
    "df_bk = df_bk.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMOTE\n",
    "y = df_bk.BK\n",
    "X = df_bk.drop('BK', axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "sm = SMOTE(random_state=27)\n",
    "X_train, y_train = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.value_counts()\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ROC curve\n",
    "def plot_roc(y_test, y_pred):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred, pos_label=1, drop_intermediate = False)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',\n",
    "             lw=lw, label='ROC curve (AUC = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([-0.001, 1.001])\n",
    "    plt.ylim([-0.001, 1.001])\n",
    "    plt.xlabel('1-Specificity (False Negative Rate)')\n",
    "    plt.ylabel('Sensitivity (True Positive Rate)')\n",
    "    plt.title('ROC curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "# Confusion Matrix returns in the format: cm[0,0], cm[0,1], cm[1,0], cm[1,1]: tn, fp, fn, tp\n",
    "\n",
    "# Sensitivity\n",
    "def custom_sensitivity_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tp/(tp+fn))\n",
    "\n",
    "# Specificity\n",
    "def custom_specificity_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tn/(tn+fp))\n",
    "\n",
    "# Positive Predictive Value\n",
    "def custom_ppv_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tp/(tp+fp))\n",
    "\n",
    "# Negative Predictive Value\n",
    "def custom_npv_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return (tn/(tn+fn))\n",
    "\n",
    "# Accuracy\n",
    "def custom_accuracy_score(y_test, y_pred):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    tn, fp, fn, tp = cm[0][0], cm[0][1], cm[1][0], cm[1][1]\n",
    "    return ((tn+tp)/(tn+tp+fn+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_confusion_matrix(cf,\n",
    "                          group_names=None,\n",
    "                          categories='auto',\n",
    "                          count=True,\n",
    "                          percent=True,\n",
    "                          cbar=True,\n",
    "                          xyticks=True,\n",
    "                          xyplotlabels=True,\n",
    "                          sum_stats=True,\n",
    "                          figsize=None,\n",
    "                          cmap='Blues',\n",
    "                          title=None):\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE TEXT INSIDE EACH SQUARE\n",
    "    blanks = ['' for i in range(cf.size)]\n",
    "\n",
    "    if group_names and len(group_names)==cf.size:\n",
    "        group_labels = [\"{}\\n\".format(value) for value in group_names]\n",
    "    else:\n",
    "        group_labels = blanks\n",
    "\n",
    "    if count:\n",
    "        group_counts = [\"{0:0.0f}\\n\".format(value) for value in cf.flatten()]\n",
    "    else:\n",
    "        group_counts = blanks\n",
    "\n",
    "    if percent:\n",
    "        group_percentages = [\"{0:.2%}\".format(value) for value in cf.flatten()/np.sum(cf)]\n",
    "    else:\n",
    "        group_percentages = blanks\n",
    "\n",
    "    box_labels = [f\"{v1}{v2}{v3}\".strip() for v1, v2, v3 in zip(group_labels,group_counts,group_percentages)]\n",
    "    box_labels = np.asarray(box_labels).reshape(cf.shape[0],cf.shape[1])\n",
    "\n",
    "\n",
    "    # CODE TO GENERATE SUMMARY STATISTICS & TEXT FOR SUMMARY STATS\n",
    "    if sum_stats:\n",
    "        #Accuracy is sum of diagonal divided by total observations\n",
    "        accuracy  = np.trace(cf) / float(np.sum(cf))\n",
    "\n",
    "        #if it is a binary confusion matrix, show some more stats\n",
    "        if len(cf)==2:\n",
    "            #Metrics for Binary Confusion Matrices\n",
    "            precision = cf[1,1] / sum(cf[:,1])\n",
    "            recall    = cf[1,1] / sum(cf[1,:])\n",
    "            f1_score  = 2*precision*recall / (precision + recall)\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\\nPrecision={:0.3f}\\nRecall={:0.3f}\\nF1 Score={:0.3f}\".format(\n",
    "                accuracy,precision,recall,f1_score)\n",
    "        else:\n",
    "            stats_text = \"\\n\\nAccuracy={:0.3f}\".format(accuracy)\n",
    "    else:\n",
    "        stats_text = \"\"\n",
    "\n",
    "\n",
    "    # SET FIGURE PARAMETERS ACCORDING TO OTHER ARGUMENTS\n",
    "    if figsize==None:\n",
    "        #Get default figure size if not set\n",
    "        figsize = plt.rcParams.get('figure.figsize')\n",
    "\n",
    "    if xyticks==False:\n",
    "        #Do not show categories if xyticks is False\n",
    "        categories=False\n",
    "\n",
    "\n",
    "    # MAKE THE HEATMAP VISUALIZATION\n",
    "    plt.figure(figsize=figsize)\n",
    "    sns.heatmap(cf,annot=box_labels,fmt=\"\",cmap=cmap,cbar=cbar,xticklabels=categories,yticklabels=categories)\n",
    "\n",
    "    if xyplotlabels:\n",
    "        plt.ylabel('True label')\n",
    "        plt.xlabel('Predicted label' + stats_text)\n",
    "    else:\n",
    "        plt.xlabel(stats_text)\n",
    "    \n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search with cross validation\n",
    "\n",
    "#Score_func defines the performance measure(auc) which the gridsearchCV should use\n",
    "score_func = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "\n",
    "# Create a parameter grid to test various hyper parameter values\n",
    "param_grid_rf = {\n",
    "    'max_depth': [80, 100, 110],\n",
    "    'max_features': [2, 3],\n",
    "    'min_samples_leaf': [3, 4, 5],\n",
    "    'min_samples_split': [8, 10, 12],\n",
    "    'n_estimators': [50, 100, 300]\n",
    "}\n",
    "\n",
    "# Defining the Random Forest Classifier model\n",
    "classifier_RF = RandomForestClassifier()\n",
    "\n",
    "# Hyper-parameter tuning (\"optimization\") using the function GridSearchCV for maximizing AUC\n",
    "# 5-fold cross-validation\n",
    "# Instantiate the grid search model\n",
    "grid_search_rf = GridSearchCV(estimator = classifier_RF, param_grid = param_grid_rf, \n",
    "                          cv = 5, scoring = score_func, n_jobs=-1,return_train_score = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model to the training dataset\n",
    "grid_search_RF = grid_search_rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out which are the best hyper parameter values where auc for the model is highest\n",
    "print('\\nBest Hyper-Parameter values Random Forest:'+str(grid_search_RF.best_params_))\n",
    "grid_search_RF.best_params_\n",
    "\n",
    "#Best Estimator for Random Forest Model\n",
    "best_grid_rf = grid_search_RF.best_estimator_\n",
    "best_grid_rf\n",
    "\n",
    "#Score of the best model\n",
    "best_result_rf = grid_search_RF.best_score_\n",
    "print(\"\\nBest Score Random Forest: \" + str(best_result_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the above (best) model with the best hyper parameter values to predict the testing data\n",
    "class_threshold = 0.50\n",
    "y_pred_prob_rf = grid_search_RF.predict_proba(X_test)[:,1]\n",
    "y_pred_rf = np.where(y_pred_prob_rf > class_threshold, 1, 0) # classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Measure\n",
    "\n",
    "#Confusion Matrix\n",
    "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
    "print(\"\\nConfusion matrix Random Forest: \\n\" + str(cm_rf))\n",
    "\n",
    "#Auc Calculation\n",
    "auc_rf=roc_auc_score(y_test, y_pred_prob_rf)\n",
    "print(\"\\nAUC Random Forest: \" +str(auc_rf))\n",
    "\n",
    "#ROC plot\n",
    "plot_roc(y_test, y_pred_prob_rf)\n",
    "\n",
    "#Other Performance Metrics\n",
    "print(\"                                   Accuracy Random Forest: \" + str(custom_accuracy_score(y_test, y_pred_rf))) \n",
    "print(\"                   SENSITIVITY (aka RECALL) Random Forest: \" + str(custom_sensitivity_score(y_test, y_pred_rf)))\n",
    "print(\"                 SPECIFICITY (aka FALL-OUT) Random Forest: \" + str(custom_specificity_score(y_test, y_pred_rf)))\n",
    "print(\" POSITIVE PREDICTIVE VALUE, (aka PRECISION) Random Forest: \" + str(custom_ppv_score(y_test, y_pred_rf)))\n",
    "print(\"                 NEGATIVE PREDICTIVE VALUE) Random Forest: \" + str(custom_npv_score(y_test, y_pred_rf)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "#class_names=[str(x) for x in classifier_RF.]\n",
    "print(classification_report(y_test,y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "categories = [\"Zero\", \"One\"]\n",
    "make_confusion_matrix(cm_rf, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Importance - Using Random Forest Model\n",
    "importances = grid_search_RF.best_estimator_.feature_importances_ \n",
    "\n",
    "#Plot the varibales according to their importance\n",
    "plt.figure(figsize=(15,5))\n",
    "plt.title('Feature Importance Random Forest')\n",
    "plt.xlabel('Decrease in Gini (recal, Gini = 2*AUC-1)')\n",
    "feature_importances = pd.Series(grid_search_RF.best_estimator_.feature_importances_ , index=X_train.columns)\n",
    "feature_importances.nlargest(17).sort_values().plot(kind='barh', align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid Search with cross validation\n",
    "\n",
    "#Score_func defines the performance measure with which the gridsearchCV should use\n",
    "score_func = make_scorer(roc_auc_score, greater_is_better=True)\n",
    "\n",
    "# Create the parameter grid to test various hyper parameters\n",
    "param_grid_logistic = {'penalty' : ['l1', 'l2'], 'C' : np.logspace(-4, 4, 20)}\n",
    "    \n",
    "#Define the Logistic model\n",
    "logistic_model = LogisticRegression(solver='liblinear')\n",
    "\n",
    "# Hyper-parameter tuning (\"optimization\") using the function GridSearchCV for maximizing AUC\n",
    "#5-fold cross-validation\n",
    "# Instantiate the grid search model\n",
    "grid_search_logistic = GridSearchCV(estimator = logistic_model, param_grid = param_grid_logistic, \n",
    "                          cv = 5, scoring = score_func, n_jobs=-1,return_train_score = True, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the model to training data\n",
    "grid_search_logistic = grid_search_logistic.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding out which are the best hyper parameter values where auc for the model is highest\n",
    "print('\\nBest Hyper-Parameter values Logistic:' + str(grid_search_logistic.best_params_)) \n",
    "grid_search_logistic.best_params_\n",
    "\n",
    "#Best Estimator for Logistic Model\n",
    "best_grid_logistic = grid_search_logistic.best_estimator_\n",
    "best_grid_logistic\n",
    "\n",
    "#Viweing the best score of the model\n",
    "best_result_logistic = grid_search_logistic.best_score_\n",
    "print(\"\\nBest Score Logistic: \"+str(best_result_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the above (best) model with the best hyper parameter values to predict the testing data\n",
    "class_threshold=0.50\n",
    "y_pred_prob_logistic = grid_search_logistic.predict_proba(X_test)[:,1] # probabilities\n",
    "y_pred_logistic = np.where(y_pred_prob_logistic > class_threshold, 1, 0) # classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Performance Measures\n",
    "#Confusion Matrix\n",
    "cm_logistic = confusion_matrix(y_test, y_pred_logistic)\n",
    "print(\"\\nConfusion matrix Logistic: \\n\" + str(cm_logistic))\n",
    "\n",
    "#Auc Calculation\n",
    "auc_logistic = roc_auc_score(y_test, y_pred_prob_logistic)\n",
    "print(\"\\nAUC Logistic:  \" + str(auc_logistic))\n",
    "\n",
    "#ROC plot\n",
    "plot_roc(y_test, y_pred_prob_logistic)\n",
    "\n",
    "#Other Performance Metrics\n",
    "print(\"                                   Accuracy Logistic: \" + str(custom_accuracy_score(y_test, y_pred_logistic))) \n",
    "print(\"                   SENSITIVITY (aka RECALL) Logistic: \" + str(custom_sensitivity_score(y_test, y_pred_logistic)))\n",
    "print(\"                 SPECIFICITY (aka FALL-OUT) Logistic: \" + str(custom_specificity_score(y_test, y_pred_logistic)))\n",
    "print(\" POSITIVE PREDICTIVE VALUE, (aka PRECISION) Logistic: \" + str(custom_ppv_score(y_test, y_pred_logistic)))\n",
    "print(\"                 NEGATIVE PREDICTIVE VALUE) Logistic: \" + str(custom_npv_score(y_test, y_pred_logistic)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test,y_pred_logistic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n",
    "categories = [\"Zero\", \"One\"]\n",
    "make_confusion_matrix(cm_logistic, \n",
    "                      group_names=labels,\n",
    "                      categories=categories, \n",
    "                      cmap='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
